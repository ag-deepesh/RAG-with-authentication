{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-xz_WgDEN1je"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install openai\n",
        "!pip -q install langchain-openai\n",
        "!pip -q install langchain-core\n",
        "!pip -q install langchain-community\n",
        "!pip -q install sentence-transformers\n",
        "!pip -q install langchain-huggingface\n",
        "!pip -q install langchain-chroma\n",
        "!pip -q install chromadb\n",
        "!pip -q install pypdf\n",
        "!pip -q install langchain\n",
        "!pip -q install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "import os\n",
        "import gradio as gr\n",
        "import openai"
      ],
      "metadata": {
        "id": "Gm8nzi4aN2nN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read OpenAI key from Colab Secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('OPENAI_KEY')           # <-- change this as per your secret's name\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "FmgU4IgjdDbg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = '/content/vtransformers_d1.pdf'\n",
        "d2 = '/content/rag_d2.pdf'\n",
        "d3 = '/content/transformers_d3.pdf'\n",
        "d4 = '/content/ai_d4.pdf'\n",
        "\n",
        "# Define user roles and document access permissions\n",
        "roles = {\n",
        "    \"admin\": [\"d1\", \"d2\", \"d3\", \"d4\"],\n",
        "    \"contributor\": [\"d1\", \"d2\", \"d3\"],\n",
        "    \"restricted-user\": [\"d3\"],\n",
        "}\n",
        "\n",
        "# Define users and their roles\n",
        "users = {\n",
        "    \"abc\": {\"role\": \"admin\", \"password\": \"abc\"},\n",
        "    \"pqr\": {\"role\": \"contributor\", \"password\": \"pqr\"},\n",
        "    \"rst\": {\"role\": \"restricted-user\", \"password\": \"rst\"},\n",
        "}\n"
      ],
      "metadata": {
        "id": "OHTFJDH3dDkP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load documents\n",
        "documents = {}\n",
        "documents[\"d1\"] = d1\n",
        "documents[\"d2\"] = d2\n",
        "documents[\"d3\"] = d3\n",
        "documents[\"d4\"] = d4\n",
        "\n",
        "# Create a single vector store for all documents\n",
        "all_docs = []\n",
        "for doc_id in documents:\n",
        "  all_docs.extend(PyPDFLoader(documents[doc_id]).load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "all_texts = text_splitter.split_documents(all_docs)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(all_texts, embeddings)"
      ],
      "metadata": {
        "id": "-1MoVUnMekLB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    k=3,\n",
        "    output_key='answer'\n",
        ")"
      ],
      "metadata": {
        "id": "Zm5g62DlstIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83d8abd-4575-40eb-deb7-e1444a1d15ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-5267b4b43d68>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qa_chain(vectordb, accessible_doc_ids):\n",
        "  accessible_docs = []\n",
        "  for doc_id in accessible_doc_ids:\n",
        "    accessible_docs.append(documents[doc_id])\n",
        "\n",
        "  retriever = vectordb.as_retriever(search_type=\"mmr\",\n",
        "      search_kwargs={\"k\": 2, \"fetch_k\":5, \"filter\": {\"source\": {\"$in\": accessible_docs}}}\n",
        "  )\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0),\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "  print(f\"qa chain: {qa_chain}\")\n",
        "  return qa_chain\n"
      ],
      "metadata": {
        "id": "9_VCmu665vFO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(username, password, query):\n",
        "    if username not in users or users[username][\"password\"] != password:\n",
        "        return \"Incorrect username or password.\", \"\", \"\"\n",
        "    user_role = users[username][\"role\"]\n",
        "    accessible_doc_ids = roles[user_role]\n",
        "\n",
        "    if not accessible_doc_ids:\n",
        "        return \"No documents available for your role.\", \"\", \"\"\n",
        "\n",
        "    qa_chain = get_qa_chain(vectordb, accessible_doc_ids)\n",
        "    result = qa_chain({\"question\": query})\n",
        "\n",
        "    # Get metadata of retrieved documents\n",
        "    retrieved_docs_metadata = \"\"\n",
        "    if 'source_documents' in result:\n",
        "        for doc in result['source_documents']:\n",
        "            if hasattr(doc, 'metadata'):\n",
        "                retrieved_docs_metadata += f\"Metadata: {doc.metadata}\\n\"\n",
        "\n",
        "    return (result[\"answer\"], user_role, retrieved_docs_metadata)"
      ],
      "metadata": {
        "id": "6kJWAgqK5vIe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Interface: Gradio"
      ],
      "metadata": {
        "id": "xnCuJtWL7sVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=process_query,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Username\"),\n",
        "        gr.Textbox(label=\"Password\", type=\"password\"),\n",
        "        gr.Textbox(label=\"Query\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Response\"),\n",
        "        gr.Textbox(label=\"User Role\"),\n",
        "        gr.Textbox(label=\"Document Metadata\")\n",
        "    ],\n",
        "    title=\"RAG System with User Access Control\",\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "_JOCjWGbooQu",
        "outputId": "71f042ff-0290-4fad-d268-7fa995e54ff7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eaa020de1d602d5e74.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eaa020de1d602d5e74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404e4d30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e80404e4a00>, root_client=<openai.OpenAI object at 0x7e80406a1060>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404e5510>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404e4d30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e80404e4a00>, root_client=<openai.OpenAI object at 0x7e80406a1060>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404e5510>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf', '/content/ai_d4.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80405e9ea0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e804054b010>, root_client=<openai.OpenAI object at 0x7e8040548490>, root_async_client=<openai.AsyncOpenAI object at 0x7e80405e9f00>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80405e9ea0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e804054b010>, root_client=<openai.OpenAI object at 0x7e8040548490>, root_async_client=<openai.AsyncOpenAI object at 0x7e80405e9f00>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working mechanism:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a 1D vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This includes multiple layers of self-attention and feed-forward neural networks, allowing the model to learn relationships between different patches.\\n\\n5. **Classification Head**: For tasks like image classification, a special classification token is added to the sequence of patch embeddings. After processing through the transformer layers, the output corresponding to this token is used for classification.\\n\\n6. **Output**: The final output can be used for various vision tasks, such as classification, detection, or segmentation, depending on how the model is configured.\\n\\nThis mechanism allows Vision Transformers to capture global context and relationships in images effectively, leading to their strong performance in various vision tasks.', additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e8040547100>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562230>, root_client=<openai.OpenAI object at 0x7e8040560f40>, root_async_client=<openai.AsyncOpenAI object at 0x7e8040546c80>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e8040547100>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562230>, root_client=<openai.OpenAI object at 0x7e8040560f40>, root_async_client=<openai.AsyncOpenAI object at 0x7e8040546c80>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/transformers_d3.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working mechanism:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a 1D vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This includes multiple layers of self-attention and feed-forward neural networks, allowing the model to learn relationships between different patches.\\n\\n5. **Classification Head**: For tasks like image classification, a special classification token is added to the sequence of patch embeddings. After processing through the transformer layers, the output corresponding to this token is used for classification.\\n\\n6. **Output**: The final output can be used for various vision tasks, such as classification, detection, or segmentation, depending on how the model is configured.\\n\\nThis mechanism allows Vision Transformers to capture global context and relationships in images effectively, leading to their strong performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404354b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562950>, root_client=<openai.OpenAI object at 0x7e80404376a0>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404373d0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404354b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562950>, root_client=<openai.OpenAI object at 0x7e80404376a0>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404373d0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working mechanism:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a 1D vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This includes multiple layers of self-attention and feed-forward neural networks, allowing the model to learn relationships between different patches.\\n\\n5. **Classification Head**: For tasks like image classification, a special classification token is added to the sequence of patch embeddings. After processing through the transformer layers, the output corresponding to this token is used for classification.\\n\\n6. **Output**: The final output can be used for various vision tasks, such as classification, detection, or segmentation, depending on how the model is configured.\\n\\nThis mechanism allows Vision Transformers to capture global context and relationships in images effectively, leading to their strong performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches rather than as 2D grids of pixels. Here’s a breakdown of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. For example, a 224x224 image might be split into 16x16 patches, resulting in a sequence of patches.\\n\\n2. **Linear Embedding**: Each patch is then flattened into a vector and passed through a linear layer to create an embedding for each patch. This transforms the patches into a format suitable for processing by the transformer model.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. This helps the model to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This typically consists of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the model to weigh the importance of different patches relative to each other, enabling it to capture global context and relationships within the image.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically pooled (e.g., using the output of a specific patch designated as the classification token) and passed through a classification head (usually a feed-forward neural network) to produce the final predictions.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, such as their ability to model long-range dependencies, while effectively handling image data.', additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80408cf190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e80406efe20>, root_client=<openai.OpenAI object at 0x7e80406efb80>, root_async_client=<openai.AsyncOpenAI object at 0x7e80408cf160>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80408cf190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e80406efe20>, root_client=<openai.OpenAI object at 0x7e80406efb80>, root_async_client=<openai.AsyncOpenAI object at 0x7e80408cf160>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working mechanism:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a 1D vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This includes multiple layers of self-attention and feed-forward neural networks, allowing the model to learn relationships between different patches.\\n\\n5. **Classification Head**: For tasks like image classification, a special classification token is added to the sequence of patch embeddings. After processing through the transformer layers, the output corresponding to this token is used for classification.\\n\\n6. **Output**: The final output can be used for various vision tasks, such as classification, detection, or segmentation, depending on how the model is configured.\\n\\nThis mechanism allows Vision Transformers to capture global context and relationships in images effectively, leading to their strong performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches rather than as 2D grids of pixels. Here’s a breakdown of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. For example, a 224x224 image might be split into 16x16 patches, resulting in a sequence of patches.\\n\\n2. **Linear Embedding**: Each patch is then flattened into a vector and passed through a linear layer to create an embedding for each patch. This transforms the patches into a format suitable for processing by the transformer model.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. This helps the model to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This typically consists of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the model to weigh the importance of different patches relative to each other, enabling it to capture global context and relationships within the image.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically pooled (e.g., using the output of a specific patch designated as the classification token) and passed through a classification head (usually a feed-forward neural network) to produce the final predictions.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, such as their ability to model long-range dependencies, while effectively handling image data.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Give more details on the Patch part', additional_kwargs={}, response_metadata={}), AIMessage(content='The patch extraction process in Vision Transformers involves dividing an image into fixed-size patches, such as 16x16 pixels. Each of these patches is then flattened into a vector, which allows the model to process the image in a way that captures spatial information while maintaining a manageable input size for the transformer architecture. This step is crucial as it transforms the 2D image data into a format suitable for the subsequent linear projection and self-attention mechanisms used in Vision Transformers.', additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e8040547400>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040434dc0>, root_client=<openai.OpenAI object at 0x7e80406efac0>, root_async_client=<openai.AsyncOpenAI object at 0x7e8040545120>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e8040547400>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040434dc0>, root_client=<openai.OpenAI object at 0x7e80406efac0>, root_async_client=<openai.AsyncOpenAI object at 0x7e8040545120>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf']}}})\n",
            "qa chain: memory=ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Who is the Prime Minister of India', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain transformers', additional_kwargs={}, response_metadata={}), AIMessage(content='Transformers are a type of neural network architecture that was originally designed for natural language processing (NLP) tasks. They utilize mechanisms such as self-attention to process and generate sequences of data, allowing them to capture long-range dependencies and relationships within the data effectively. Transformers have gained significant attention due to their ability to achieve state-of-the-art performance in various tasks, not only in NLP but also in other fields such as image generation, music composition, and scientific research. Their versatility and robust frameworks have made them a significant advancement in AI technology.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, along with their positional encodings, is fed into a transformer model. This model consists of multiple layers of self-attention and feed-forward neural networks, allowing it to capture complex relationships and dependencies between the patches.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically passed through a classification head (e.g., a fully connected layer) to produce the final predictions for tasks such as image classification.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, which have been successful in natural language processing, to achieve state-of-the-art performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches instead of traditional 2D grids of pixels. Here’s a brief overview of their working mechanism:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. Each patch is then flattened into a 1D vector.\\n\\n2. **Linear Projection**: These flattened patches are linearly projected into a higher-dimensional space, creating patch embeddings.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This includes multiple layers of self-attention and feed-forward neural networks, allowing the model to learn relationships between different patches.\\n\\n5. **Classification Head**: For tasks like image classification, a special classification token is added to the sequence of patch embeddings. After processing through the transformer layers, the output corresponding to this token is used for classification.\\n\\n6. **Output**: The final output can be used for various vision tasks, such as classification, detection, or segmentation, depending on how the model is configured.\\n\\nThis mechanism allows Vision Transformers to capture global context and relationships in images effectively, leading to their strong performance in various vision tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the working of the vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='Vision Transformers (ViTs) work by treating images as sequences of patches rather than as 2D grids of pixels. Here’s a breakdown of their working:\\n\\n1. **Patch Extraction**: The input image is divided into fixed-size patches. For example, a 224x224 image might be split into 16x16 patches, resulting in a sequence of patches.\\n\\n2. **Linear Embedding**: Each patch is then flattened into a vector and passed through a linear layer to create an embedding for each patch. This transforms the patches into a format suitable for processing by the transformer model.\\n\\n3. **Positional Encoding**: Since transformers do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. This helps the model to retain information about the position of each patch in the original image.\\n\\n4. **Transformer Architecture**: The sequence of patch embeddings, now with positional encodings, is fed into a standard transformer architecture. This typically consists of multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows the model to weigh the importance of different patches relative to each other, enabling it to capture global context and relationships within the image.\\n\\n5. **Classification Head**: After processing through the transformer layers, the output is typically pooled (e.g., using the output of a specific patch designated as the classification token) and passed through a classification head (usually a feed-forward neural network) to produce the final predictions.\\n\\nThis approach allows Vision Transformers to leverage the strengths of transformer models, such as their ability to model long-range dependencies, while effectively handling image data.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Give more details on the Patch part', additional_kwargs={}, response_metadata={}), AIMessage(content='The patch extraction process in Vision Transformers involves dividing an image into fixed-size patches, such as 16x16 pixels. Each of these patches is then flattened into a vector, which allows the model to process the image in a way that captures spatial information while maintaining a manageable input size for the transformer architecture. This step is crucial as it transforms the 2D image data into a format suitable for the subsequent linear projection and self-attention mechanisms used in Vision Transformers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who is current Chief Minister of UP', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={})]), output_key='answer', return_messages=True, memory_key='chat_history', k=3) verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404e5000>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562e30>, root_client=<openai.OpenAI object at 0x7e80405e8f40>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404e5c60>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e80404e5000>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e8040562e30>, root_client=<openai.OpenAI object at 0x7e80405e8f40>, root_async_client=<openai.AsyncOpenAI object at 0x7e80404e5c60>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7e804ecdd210>, search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5, 'filter': {'source': {'$in': ['/content/vtransformers_d1.pdf', '/content/rag_d2.pdf', '/content/transformers_d3.pdf']}}})\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://eaa020de1d602d5e74.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}